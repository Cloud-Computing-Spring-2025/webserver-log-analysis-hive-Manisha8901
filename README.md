# Web-Server-Log-Analysis
üìå Project Overview
This project analyzes web server logs using Apache Hive. The goal is to extract insights such as:

Total Web Requests
Status Code Analysis (200, 404, 500)
Top 3 Most Visited Pages
Traffic Source Analysis (User Agents)
Detect Suspicious Activity (IPs with failed requests)
Traffic Trends Over Time
Partitioning by Status Code for Optimization

Dataset Format (CSV)
The web server log file contains the following fields:

Field Name	Data Type	Description
ip	STRING	Client IP Address
timestamp	STRING	Request Time (YYYY-MM-DD HH:MM:SS)
url	STRING	Requested URL
status	INT	HTTP Status Code (200, 404, 500, etc.)
user_agent	STRING	Browser/User Agent

192.168.1.1,2024-02-01 10:15:00,/home,200,Mozilla/5.0
192.168.1.2,2024-02-01 10:16:00,/products,200,Chrome/90.0
192.168.1.3,2024-02-01 10:17:00,/checkout,404,Safari/13.1
192.168.1.10,2024-02-01 10:18:00,/home,500,Mozilla/5.0
192.168.1.15,2024-02-01 10:19:00,/products,404,Chrome/90.0

üöÄ Implementation Steps
1Ô∏è‚É£ Setup Apache Hive with Docker
Create a docker-compose.yml file and start the services:
docker-compose up -d

2Ô∏è‚É£ Access Hive CLI
Enter the Hive shell
docker exec -it hive-server /bin/bash
hive
3Ô∏è‚É£ Create Database and Table:

CREATE DATABASE web_log_analysis;
USE web_log_analysis;

CREATE EXTERNAL TABLE web_logs (
    ip STRING,
    log_timestamp STRING,  -- Renamed "timestamp" (reserved keyword)
    url STRING,
    status INT,
    user_agent STRING
)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE 
LOCATION '/user/hive/web_logs/';
4Ô∏è‚É£ Upload Data to HDFS
First, copy the CSV file into the Namenode container:

docker cp web_logs.csv namenode:/tmp/web_logs.csv
Move it into HDFS:

docker exec -it namenode /bin/bash
hdfs dfs -mkdir -p /user/hive/web_logs
hdfs dfs -put /tmp/web_logs.csv /user/hive/web_logs/

5Ô∏è‚É£ Load Data into Hive Table

LOAD DATA INPATH '/user/hive/web_logs/web_logs.csv' INTO TABLE web_logs;
üìä Querying the Data
üîπ Count Total Web Requests
SELECT COUNT(*) AS total_requests FROM web_logs;
üîπ Status Code Analysis
SELECT status, COUNT(*) AS count FROM web_logs GROUP BY status ORDER BY count DESC;
üîπ Top 3 Most Visited Pages
SELECT url, COUNT(*) AS visit_count FROM web_logs GROUP BY url ORDER BY visit_count DESC LIMIT 3;
üîπ Traffic Source Analysis (Most Common User Agents)
SELECT user_agent, COUNT(*) AS count FROM web_logs GROUP BY user_agent ORDER BY count DESC LIMIT 3;
üîπ Detect Suspicious IPs (More Than 3 Failed Requests)
SELECT ip, COUNT(*) AS failed_requests FROM web_logs WHERE status IN (404, 500) GROUP BY ip HAVING COUNT(*) > 3;
üîπ Traffic Trends (Requests per Minute)
SELECT SUBSTR(log_timestamp, 1, 16) AS minute, COUNT(*) AS request_count FROM web_logs GROUP BY SUBSTR(log_timestamp, 1, 16) ORDER BY minute;
üóÇÔ∏è Partitioning for Optimization
Partitioning by status code improves query performance.

1Ô∏è‚É£ Create Partitioned Table
CREATE TABLE web_logs_partitioned (
    ip STRING,
    log_timestamp STRING,
    url STRING,
    user_agent STRING
)
PARTITIONED BY (status INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE;
2Ô∏è‚É£ Enable Dynamic Partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
3Ô∏è‚É£ Insert Data into Partitioned Table
INSERT OVERWRITE TABLE web_logs_partitioned PARTITION(status)
SELECT ip, log_timestamp, url, user_agent, status FROM web_logs;
üìù Challenges Faced
Reserved Keyword Issue
"timestamp" is a Hive reserved keyword ‚Üí Renamed to log_timestamp.
HDFS File Not Found Error
Ensured file was uploaded to /user/hive/web_logs/ before loading into Hive.
Slow Queries
Used Partitioning on status to optimize query performance.
üìå Sample Output
1Ô∏è‚É£ Total Web Requests
Total Requests: 100
2Ô∏è‚É£ Status Code Analysis
makefile
200: 80
404: 10
500: 10
3Ô∏è‚É£ Most Visited Pages
/home: 50
/products: 30
/checkout: 20
4Ô∏è‚É£ Suspicious IPs
makefile
192.168.1.10: 5 failed requests
192.168.1.15: 4 failed requests




